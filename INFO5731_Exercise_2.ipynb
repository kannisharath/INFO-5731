{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kannisharath/INFO-5731/blob/main/INFO5731_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANSWER:\n",
        "The research question is \"The impact of remote work on employee productivity and job satisfaction across different industries\".\n",
        "The data need to be collected to answer this question is Employee productivity,Job satisfaction, Industry and Demographic imformation such as age and gender.Here productivity and job satisfaction are measured on a scale 1-100.\n",
        "\n",
        "The amount of data needed would depend on the number of industries being analyzed. A large enough sample size is crucial to ensure statistical significance and generalizability of findings. Aim for hundreds to thousands of employees across various industries for analysis.\n",
        "\n",
        "The detail steps for collecting and saving data is\n",
        "1) Identifying target industries.\n",
        "2) Create reliable data collection tools.\n",
        "3) Conduct surveys using email, internet platforms, or intranets.\n",
        "4) Get productivity metrics from tools or databases.\n",
        "5) Manage data securely.\n",
        "6) Use statistics to analyze data."
      ],
      "metadata": {
        "id": "vIPT7HTK12Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing pandas and numpy modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Defining a function to generate synthetic data\n",
        "def generate_data(num_samples):\n",
        "    # Generating random productivity scores ranges from 0 and 100\n",
        "    productivity_scores = np.random.randint(0, 101, size=num_samples)\n",
        "    # Generating random job satisfaction scores ranges  0 and 100\n",
        "    job_satisfaction_scores = np.random.randint(0, 101, size=num_samples)\n",
        "    # Generating random industry categories\n",
        "    industries = np.random.choice(['IT', 'Finance', 'Healthcare', 'Manufacturing'], size=num_samples)\n",
        "    # Generating random demographic information such as age\n",
        "    ages = np.random.randint(20, 60, size=num_samples)\n",
        "    genders = np.random.choice(['Male', 'Female'], size=num_samples)\n",
        "    # Creating a DataFrame\n",
        "    data = pd.DataFrame({\n",
        "        'Productivity': productivity_scores,\n",
        "        'Job_Satisfaction': job_satisfaction_scores,\n",
        "        'Industry': industries,\n",
        "        'Age': ages,\n",
        "        'Gender': genders\n",
        "    })\n",
        "\n",
        "    return data\n",
        "# Generating a dataset with 1000 samples\n",
        "num_samples = 1000\n",
        "dataset = generate_data(num_samples)\n",
        "# Save dataset to CSV file\n",
        "dataset.to_csv('work_dataset.csv', index=False)\n",
        "\n",
        "print(\"Dataset saved successfully.\")"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75879d0a-c44e-4614-da70-2af367098c62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "topic = 'paper'\n",
        "url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "articles_limit = 1000\n",
        "articles_per_request = 100\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "for offset in range(0, articles_limit, articles_per_request):\n",
        "    params = {\n",
        "        'query': topic,\n",
        "        'offset': offset,\n",
        "        'limit': articles_per_request,\n",
        "        'fields': 'title,venue,year,abstract,authors.name'\n",
        "    }\n",
        "\n",
        "    headers = {}\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        papers = data.get('data', [])\n",
        "\n",
        "        # Create a DataFrame for the current batch\n",
        "        df_batch = pd.DataFrame([\n",
        "            {\n",
        "                'paperId': paper['paperId'],\n",
        "                'title': paper['title'],\n",
        "                'abstract': paper.get('abstract', ''),\n",
        "                'venue': paper.get('venue', ''),\n",
        "                'year': paper.get('year', ''),\n",
        "                'authors': [author['name'] for author in paper.get('authors', [])]\n",
        "            }\n",
        "            for paper in papers\n",
        "        ])\n",
        "\n",
        "        # Append the DataFrame to the list\n",
        "        dfs.append(df_batch)\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        break  # Break the loop in case of an error\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single DataFrame\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Filter articles by year range (2014 to 2024)\n",
        "df = df[df['year'].between(2014, 2024)]\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_filename = 'papers_data.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(\"code executed sucessfully\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrI9mUHM_yET",
        "outputId": "b9b7a2b7-c7fd-49ba-c92a-786d1aa93916"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code executed sucessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "306021fb-41d6-4f63-844c-8cc40cd8cf42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/191.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/191.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.7.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "reddit_instance = praw.Reddit(client_id='Rm51RgkkORs6zhyg23rNcg',\n",
        "                              client_secret='G4yNrSb24cbkLUXm5frLVOxjji18Bg',\n",
        "                              user_agent='test')\n",
        "\n",
        "\n",
        "def get_reddit_entries(subreddit_name, search_query, entry_limit=100):\n",
        "    entries_list = []\n",
        "    target_subreddit = reddit_instance.subreddit(subreddit_name)\n",
        "\n",
        "    # Fetch submissions in the specified subreddit\n",
        "    for submission in target_subreddit.search(search_query, limit=entry_limit):\n",
        "        entry_details = {\n",
        "            'title': submission.title,\n",
        "            'url': submission.url,\n",
        "            'created_utc': datetime.datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'upvotes': submission.score,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'author': str(submission.author)\n",
        "        }\n",
        "        entries_list.append(entry_details)\n",
        "\n",
        "    return entries_list\n",
        "\n",
        "\n",
        "target_subreddit_name = 'NLP'\n",
        "search_query = 'nlp'\n",
        "entries = get_reddit_entries(target_subreddit_name, search_query, entry_limit=10)\n",
        "\n",
        "# Convert entries to DataFrame\n",
        "df_entries = pd.DataFrame(entries)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_filename = 'reddit_entries_data.csv'\n",
        "df_entries.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Print the retrieved entries\n",
        "print(\"Retrieved entries:\")\n",
        "print(df_entries)\n",
        "print(f\"\\nData saved to {csv_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtBEsHx4SaMM",
        "outputId": "cd29ac7e-f5cd-4dc3-94da-66cad90c7cea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved entries:\n",
            "                                               title  \\\n",
            "0  Why the \"NLP\" that you were taught is not real...   \n",
            "1                                    NLP Power Words   \n",
            "2           NLP For Porn Addiction & Success In Life   \n",
            "3                            NLP strategies for ADHD   \n",
            "4                                      NLP Excercise   \n",
            "5                                     NLP for comedy   \n",
            "6                             NLP for claustrophobia   \n",
            "7                                  Resources for NLP   \n",
            "8                                   NLP for dating ?   \n",
            "9                               Will NLP work on me?   \n",
            "\n",
            "                                                 url          created_utc  \\\n",
            "0                    https://v.redd.it/g1mppkz1kmqb1  2023-09-26 16:14:43   \n",
            "1  https://www.reddit.com/r/NLP/comments/1ahk6lx/...  2024-02-03 01:30:25   \n",
            "2  https://www.reddit.com/r/NLP/comments/1722kyz/...  2023-10-07 09:53:46   \n",
            "3  https://www.reddit.com/r/NLP/comments/18mhsie/...  2023-12-20 01:02:40   \n",
            "4  https://www.reddit.com/r/NLP/comments/1aspzd9/...  2024-02-17 01:42:21   \n",
            "5  https://www.reddit.com/r/NLP/comments/1acd1ux/...  2024-01-27 15:18:39   \n",
            "6  https://www.reddit.com/r/NLP/comments/184wq1n/...  2023-11-27 06:44:23   \n",
            "7  https://www.reddit.com/r/NLP/comments/19479m8/...  2024-01-11 17:32:03   \n",
            "8  https://www.reddit.com/r/NLP/comments/174vm5j/...  2023-10-10 20:42:31   \n",
            "9  https://www.reddit.com/r/NLP/comments/187avb6/...  2023-11-30 05:29:03   \n",
            "\n",
            "   upvotes  num_comments                author  \n",
            "0        0            20       JoostvanderLeij  \n",
            "1        5            15            ihatewands  \n",
            "2        6            69   Illustrious_Car5155  \n",
            "3        4            20  Capable-Breakfast480  \n",
            "4        1             3              tinse808  \n",
            "5        9             9  Environmental_Shoe80  \n",
            "6        1            15       Open-Sector2341  \n",
            "7        4            10       PrincipleOk6557  \n",
            "8        1            23          Vivid-Ad7048  \n",
            "9       11            12       Parade2thegrave  \n",
            "\n",
            "Data saved to reddit_entries_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "It took more time to complete assignment. I found question 3&4 quite difficult. I tried for 3 days to complete that questions by learning the concepts of web scrapping.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}